The results of test three, which varies packet loss probability p, show little helpful information. The liner regression line is very slighly negitive, which implies that increasing the packet loss probility decreases average delay. This is unexpected, and likely a result of random variance (although I did 20 runs at each value of p, which I hoped would average out random variance).

Again, the corrolatoin between delay and p is quite weak, so there's not much to conclude here. I suspect that the range of p is too restricted to show good results. If p ranged between .1 and .5, I suspect that the linear regression line would be possitive, which would show that delay increases as p increases.
